{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2110433 - Computer Vision (2023/2)\n",
    "## Lab 4 - Color Processing\n",
    "In this lab, we will learn to use color feature in different color spaces to extract useful information from images. This notebook includes both coding and written questions. Please hand in this notebook file with all outputs and your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OpenCV, Numpy and Matplotlib as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import random as rng\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact,interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML,clear_output\n",
    "import IPython.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grayscale Image Thresholding\n",
    "Thresholding is the simplest method of image segmentation. This non-linear operation converts a grayscale image into a binary image where the two colors (black/white) are assigned to pixels that are below or above the specified threshold. <br>\n",
    "Lena comes again! Can you adjust both sliders to segment lena's skin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c2be30803644ddbbd0b67009cd7a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='minValue', max=255), IntSlider(value=1, description='maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputImage = cv2.imread(\"assets/lena_std.tif\",cv2.IMREAD_GRAYSCALE)\n",
    "def grayscaleThresholding(minValue,maxValue):\n",
    "    thresholdImage = np.logical_and(inputImage > minValue, inputImage < maxValue)\n",
    "    inputImageCopy = inputImage.copy()\n",
    "    cv2.rectangle(inputImageCopy,(250,400),(340,500),255,3)\n",
    "    cropRegion = inputImage[400:500,250:340]\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Lena Image\")\n",
    "    plt.imshow(inputImageCopy, cmap='gray')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Segmentation Mask\")\n",
    "    plt.imshow(thresholdImage, cmap='gray')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Pixel Value Distribution\")\n",
    "    plt.hist(cropRegion,range=(0,255))\n",
    "    plt.show()\n",
    "interact(grayscaleThresholding, minValue=widgets.IntSlider(min=0,max=255,step=1,value=1),maxValue=widgets.IntSlider(min=0,max=255,step=1,value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Image Segmentation using Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above sample, only grayscale information is usually not enough to segment \"things\" from the images. In this section we will apply simple color segmentation on various colorspaces. The following block is code snippet which retrive image from your webcam and apply thresholding on BGR image using defined value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bMin = 50; bMax = 170\n",
    "gMin = 70; gMax = 180\n",
    "rMin = 90; rMax = 220\n",
    "cameraNo = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "# You can press \"Interupt Kernel Button to stop webcam\"\n",
    "inputStream = cv2.VideoCapture(cameraNo) \n",
    "try:\n",
    "    while True:\n",
    "        _, videoFrameBGR = inputStream.read()\n",
    "        if videoFrameBGR is not None:\n",
    "            outputVideoFrameBGR = videoFrameBGR.copy()\n",
    "\n",
    "            # Draw ROI\n",
    "            cv2.rectangle(outputVideoFrameBGR,(100,100),(200,200),(0,255,0),3)\n",
    "            # Cropped Region\n",
    "            croppedRegion = videoFrameBGR[100:200,100:200,:]\n",
    "            \n",
    "           \n",
    "            \n",
    "            mask = cv2.inRange(videoFrameBGR,(bMin,gMin,rMin),(bMax,gMax,rMax))[:,:,np.newaxis]\n",
    "            mask = np.repeat(mask,3,axis=2)\n",
    "            \n",
    "            ## Draw Min/Max pixel value in BGR order on image\n",
    "            cv2.putText(outputVideoFrameBGR,str(np.min(croppedRegion[:,:,0]))+'/'+str(np.min(croppedRegion[:,:,1]))+'/'+str(np.min(croppedRegion[:,:,2])),(20,20),cv2.FONT_HERSHEY_SIMPLEX,1.0,(0,0,255))\n",
    "            cv2.putText(outputVideoFrameBGR,str(np.max(croppedRegion[:,:,0]))+'/'+str(np.max(croppedRegion[:,:,1]))+'/'+str(np.max(croppedRegion[:,:,2])),(20,50),cv2.FONT_HERSHEY_SIMPLEX,1.0,(0,0,255))\n",
    "            \n",
    "            outputVideoFrameBGR = np.hstack((outputVideoFrameBGR,mask))\n",
    "            \n",
    "            # Encode image as jpg numpy array\n",
    "            _, buf = cv2.imencode(\".jpg\", outputVideoFrameBGR)\n",
    "            # Draw result\n",
    "            IPython.display.display(IPython.display.Image(data=buf))\n",
    "        \n",
    "            clear_output(wait=True) \n",
    "        else:\n",
    "            print(\"Cannot Open Webcam, hw problem?\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Stream stopped\")\n",
    "inputStream.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the slider widget does not support for-loop webcam retrival method that we use, we may use build-in OpenCV GUI library to create a color range slider by using the following code. (The window name <b>\"Color Segmentation\"</b> will popup!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliderCallback(x):\n",
    "    pass\n",
    "# Create a OpenCV Window\n",
    "windowName = 'Color Segmentation'\n",
    "cv2.namedWindow(windowName)\n",
    "cv2.createTrackbar('bMin',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('gMin',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('rMin',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('bMax',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('gMax',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('rMax',windowName,0,255,sliderCallback)\n",
    "\n",
    "inputStream = cv2.VideoCapture(cameraNo) \n",
    "try:\n",
    "    while True:\n",
    "        _, videoFrameBGR = inputStream.read()\n",
    "        if videoFrameBGR is not None:\n",
    "            \n",
    "            \n",
    "            bMin = cv2.getTrackbarPos('bMin',windowName)\n",
    "            gMin = cv2.getTrackbarPos('gMin',windowName)\n",
    "            rMin = cv2.getTrackbarPos('rMin',windowName)\n",
    "            \n",
    "            bMax = cv2.getTrackbarPos('bMax',windowName)\n",
    "            gMax = cv2.getTrackbarPos('gMax',windowName)\n",
    "            rMax = cv2.getTrackbarPos('rMax',windowName)\n",
    "            \n",
    "            mask = cv2.inRange(videoFrameBGR,(bMin,gMin,rMin),(bMax,gMax,rMax))[:,:,np.newaxis]\n",
    "            mask = np.repeat(mask,3,axis=2)\n",
    "            outputVideoFrameBGR = videoFrameBGR.copy()\n",
    "            outputVideoFrameBGR = np.hstack((outputVideoFrameBGR,mask))\n",
    "            \n",
    "            cv2.imshow(windowName,outputVideoFrameBGR)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "        else:\n",
    "            print(\"Cannot Open Webcam, hw problem?\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Stream stopped\")\n",
    "inputStream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV supports many well-known colorspaces. You can apply the colorspace transformation by using <a href=\"https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#ga397ae87e1288a81d2363b61574eb8cab\">cv2.cvtColor</a> and see the list of suppoted transformation flags <a href=\"https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#ga4e0972be5de079fed4e3a10e24ef5ef0\">here</a>. Try tp apply color segmention on any object in other colorspace <b>(NOT BGR!!)</b> by fill the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL HERE ###\n",
    "# Create a OpenCV Window\n",
    "windowName = 'Color Segmentation'\n",
    "cv2.namedWindow(windowName)\n",
    "cv2.createTrackbar('hMin',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('sMin',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('vMin',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('hMax',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('sMax',windowName,0,255,sliderCallback)\n",
    "cv2.createTrackbar('vMax',windowName,0,255,sliderCallback)\n",
    "\n",
    "inputStream = cv2.VideoCapture(cameraNo) \n",
    "try:\n",
    "    while True:\n",
    "        _, videoFrameBGR = inputStream.read()\n",
    "        if videoFrameBGR is not None:\n",
    "            \n",
    "            videoFrameHSV = cv2.cvtColor(videoFrameBGR, cv2.COLOR_BGR2HSV)\n",
    "            hMin = cv2.getTrackbarPos('hMin',windowName)\n",
    "            sMin = cv2.getTrackbarPos('sMin',windowName)\n",
    "            vMin = cv2.getTrackbarPos('vMin',windowName)\n",
    "            \n",
    "            hMax = cv2.getTrackbarPos('hMax',windowName)\n",
    "            sMax = cv2.getTrackbarPos('sMax',windowName)\n",
    "            vMax = cv2.getTrackbarPos('vMax',windowName)\n",
    "            \n",
    "            mask = cv2.inRange(videoFrameHSV,(hMin,sMin,vMin),(hMax,sMax,vMax))[:,:,np.newaxis]\n",
    "            mask = np.repeat(mask,3,axis=2)\n",
    "            outputVideoFrameHSV = videoFrameHSV.copy()\n",
    "            outputVideoFrameHSV = np.hstack((outputVideoFrameHSV,mask))\n",
    "            outputVideoFrameBGR = cv2.cvtColor(outputVideoFrameHSV, cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            cv2.imshow(windowName,outputVideoFrameBGR)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "        else:\n",
    "            print(\"Cannot Open Webcam, hw problem?\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Stream stopped\")\n",
    "inputStream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of mathematical morphology contributes a wide range of operators to image processing, all based around a simple mathematical concepts from set theory. Morphological transformations are the operations based on the image shape employed on binay images. This operation needs needs two inputs, one is binary image, second one is called <b>structuring element or kernel</b> which decides the operation output. You can design the kernel to suit your application needs. Two basic morphological operators are Erosion and Dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following mask image is segmented by using color information. You can see that there are some hand's pixels which are not connect into a perfect hand shape. We can correct these by using the basic morphological operaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handMask = cv2.imread('assets/Lab4-SegmentedHand.png',cv2.IMREAD_GRAYSCALE)\n",
    "plt.title('Segmented Hand Mask')\n",
    "plt.imshow(handMask,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef55889a69d840cc8a753010e003df4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='kernelSize', max=11, min=1), Dropdown(description='kerneâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def openAndCloseMorph(kernelSize,kernelShape, morphType):\n",
    "    kernel = cv2.getStructuringElement(kernelShape,(kernelSize,kernelSize))\n",
    "\n",
    "    outputImage = handMask.copy()\n",
    "    \n",
    "    if morphType == 'erode':\n",
    "        outputImage = cv2.erode(outputImage,kernel,iterations = 1)\n",
    "    else:\n",
    "        outputImage = cv2.dilate(outputImage,kernel,iterations = 1)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(outputImage, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Morphology Kernel Shape:')\n",
    "    display(kernel)\n",
    "    \n",
    "interact(openAndCloseMorph, kernelSize=widgets.IntSlider(min=1,max=11,step=1,value=1), \n",
    "         kernelShape=widgets.Dropdown(\n",
    "                        options=[cv2.MORPH_RECT,cv2.MORPH_ELLIPSE, cv2.MORPH_CROSS],\n",
    "                        value=cv2.MORPH_RECT,\n",
    "                        description='kernelShape:',\n",
    "                        disabled=False),\n",
    "        morphType=widgets.Dropdown(\n",
    "                        options=['erode','dilate'],\n",
    "                        value='erode',\n",
    "                        description='Morph Type:',\n",
    "                        disabled=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This <a href=\"https://docs.opencv.org/3.4.2/d9/d61/tutorial_py_morphological_ops.html\">page</a> shows a good morphological operation exmple, try to write an interactive visualization like the above sample on <b>Opening and Closing</b> operations. See the output results by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17cbb6de06a452f965ae21c07213524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='kernelSize', max=11, min=1), Dropdown(description='kerneâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FILL HERE ###\n",
    "def openAndCloseMorph(kernelSize,kernelShape):\n",
    "    kernel = cv2.getStructuringElement(kernelShape,(kernelSize,kernelSize))\n",
    "\n",
    "    outputImageOpen = handMask.copy()\n",
    "    outputImageClose = handMask.copy()\n",
    "    \n",
    "    outputImageOpen = cv2.morphologyEx(outputImageOpen, cv2.MORPH_OPEN,kernel)\n",
    "\n",
    "    outputImageClose = cv2.morphologyEx(outputImageClose, cv2.MORPH_CLOSE,kernel)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Open\")\n",
    "    plt.imshow(outputImageOpen, cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Close\")\n",
    "    plt.imshow(outputImageClose, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "interact(openAndCloseMorph, kernelSize=widgets.IntSlider(min=1,max=11,step=1,value=1), \n",
    "         kernelShape=widgets.Dropdown(\n",
    "                        options=[cv2.MORPH_RECT,cv2.MORPH_ELLIPSE, cv2.MORPH_CROSS],\n",
    "                        value=cv2.MORPH_RECT,\n",
    "                        description='kernelShape:',\n",
    "                        disabled=False),\n",
    ");\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Color Based Face Detector <br>\n",
    "\n",
    "<img src=\"assets/Lab4-funnyface.gif\"/>\n",
    "By using the knowledge from lecture 1-4, you should be able to write your own simple color based face detector. Use the above code snippets to help you write it. The output should be a code which retrive video feed from <b>your webcam</b> and draw bounding boxes around detected faces. Write the detection results into video file and hand in with this notebook. There should be <b>two video sequences</b>, in good lighting and other lighting condition. The output video should show robustness of your designed alogorithm. (Optional) You will get extra points if you can use <b>same parameters</b> for both sequences.\n",
    "\n",
    "<b>Basic Guidance:<b>\n",
    "1. Create a \"face color segmentation mask\" using your choice colorspace.\n",
    "2. Filter out the outlier pixel!\n",
    "3. Categorize each connected component into group by using cv2.findContours (from Lab 3)\n",
    "4. Find the bounding box which can enclose those connect components by <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#gacb413ddce8e48ff3ca61ed7cf626a366\">cv2.boundingRect</a>\n",
    "\n",
    "<b>Hints:</b>\n",
    "- From today lecture, how do to discard noise/fill small hole from color segmentation mask output?\n",
    "- Since this is a color-based problem, you can use old knowledge from lecture 1-3 to improve segmentation result by apply <b>?</b> on input image\n",
    "- You can use some specific threshold based on shape properties or simple morphological operations to keep only potential contours\n",
    "- To achieve a better result for both lighting conditions, you may need to apply some data analysis on the <b>region of interest</b> by plotting each channel value and see their data distributions.\n",
    "- Internet is your friend. You can search for relavent research papers and use their algorithms/implementations, but you must <b>give proper credits</b> by citing them in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Describe how your algorithm work here (Thai or English). You can provide any visualization if you want.\n",
    "'''\n",
    "1. Segmentation image in HSV color space\n",
    "2. Using Edge Detection and find contours\n",
    "3. Use close morphing (because it's return best result for my case)\n",
    "4. Drawing the biggest contour\n",
    "5. Draw Rectangles on image from contour\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL HERE ###\n",
    "class CropLayer(object):\n",
    "    def __init__(self, params, blobs):\n",
    "        self.xstart = 0\n",
    "        self.xend = 0\n",
    "        self.ystart = 0\n",
    "        self.yend = 0\n",
    "        \n",
    "    def getMemoryShapes(self, inputs):\n",
    "        inputShape, targetShape = inputs[0], inputs[1]\n",
    "        batchSize, numChannels = inputShape[0], inputShape[1]\n",
    "        height, width = targetShape[2], targetShape[3]\n",
    "\n",
    "        self.ystart = (inputShape[2] - targetShape[2]) // 2\n",
    "        self.xstart = (inputShape[3] - targetShape[3]) // 2\n",
    "        self.yend = self.ystart + height\n",
    "        self.xend = self.xstart + width\n",
    "\n",
    "        return [[batchSize, numChannels, height, width]]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return [inputs[0][:,:,self.ystart:self.yend,self.xstart:self.xend]]\n",
    "\n",
    "cv2.dnn_registerLayer('Crop', CropLayer)\n",
    "\n",
    "hedNet = cv2.dnn.readNet('assets/hed_pretrained_bsds.caffemodel', 'assets/deploy.prototxt')\n",
    "# Create a OpenCV Window\n",
    "windowName = 'Color Segmentation'\n",
    "cv2.namedWindow(windowName)\n",
    "\n",
    "videoFrames = []\n",
    "inputStream = cv2.VideoCapture(cameraNo) \n",
    "try:\n",
    "    while True:\n",
    "        _, videoFrameBGR = inputStream.read()\n",
    "        if videoFrameBGR is not None:\n",
    "            \n",
    "            videoFrameHSV = cv2.cvtColor(videoFrameBGR, cv2.COLOR_BGR2HSV)\n",
    "            hMin = 47\n",
    "            sMin = 35 \n",
    "            vMin = 29 \n",
    "            \n",
    "            hMax = 134 \n",
    "            sMax = 151 \n",
    "            vMax = 150 \n",
    "            \n",
    "            mask = cv2.inRange(videoFrameHSV,(hMin,sMin,vMin),(hMax,sMax,vMax))[:,:,np.newaxis]\n",
    "            mask = np.repeat(mask,3,axis=2)\n",
    "            kernel = cv2.getStructuringElement(2,(8, 8))\n",
    "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE,kernel)\n",
    "            \n",
    "            # Find Edges\n",
    "            maskTensor = cv2.dnn.blobFromImage(mask, scalefactor=1.0, size=(mask.shape[1], mask.shape[0]), mean=(104.00698793, 116.66876762, 122.67891434), swapRB=True, crop=False)\n",
    "            hedNet.setInput(maskTensor)\n",
    "            hedNetOutput = cv2.resize(hedNet.forward().squeeze(), (mask.shape[1], mask.shape[0]))\n",
    "\n",
    "            # Find Biggest Contour\n",
    "            contours, _ =  cv2.findContours(np.uint8(hedNetOutput > 0.4), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            biggest = max(contours, key= cv2.contourArea)\n",
    "\n",
    "            outputVideoFrameBGR = videoFrameBGR.copy()\n",
    "            color = (0, 244, 54)\n",
    "            x, y, w, h = cv2.boundingRect(biggest)\n",
    "            cv2.rectangle(outputVideoFrameBGR, (x,y), (x+w,y+h), color, 2)\n",
    "            \n",
    "            outputVideoFrameBGR = np.hstack((outputVideoFrameBGR,videoFrameBGR))\n",
    "\n",
    "            \n",
    "            videoFrames.append(outputVideoFrameBGR)\n",
    "            \n",
    "            cv2.imshow(windowName,outputVideoFrameBGR)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "        else:\n",
    "            print(\"Cannot Open Webcam, hw problem?\")\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Stream stopped\")\n",
    "inputStream.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "inputWidth, inputHeight = videoFrames[0].shape[1], videoFrames[0].shape[0]\n",
    "outputStream = cv2.VideoWriter('mark2.mp4',\n",
    "                               cv2.VideoWriter_fourcc('x', '2', '6', '4'),\n",
    "                               25, (inputWidth, inputHeight))\n",
    "\n",
    "for frame in videoFrames:\n",
    "        # Write frame to outputStream\n",
    "        outputStream.write(frame)\n",
    "        # Encode image as jpg numpy array\n",
    "        clear_output(wait=True)\n",
    "outputStream.release()\n",
    "del hedNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Invisibility Cloak \n",
    "By using the knowledge from lecture 1-4, you should be able to mimic a invisibility cloak from famous Harry Potter franchise by using color segmentation<br>\n",
    "<img src=\"assets/Lab4-invisibilitycloak.gif\"><br>\n",
    "<b>Basic Guidance:<b>\n",
    "1. Create a \"invisibility cloak color segmentation mask\" using your choice colorspace.\n",
    "2. Filter out the outlier pixel using some specify (you can think by your own!) criteria.\n",
    "3. Replace each invisible cloak area with store static background image.\n",
    "4. Make a short video clip to demonstrate/show your algorithm\n",
    "\n",
    "<b>Hints:</b>\n",
    "- From today lecture, how do to discard noise/fill small hole from color segmentation mask output?\n",
    "- Since this is a color-based problem, you can use old knowledge from lecture 1-3 to improve segmentation result by apply <b>?</b> on input image\n",
    "- Internet is your friend. You can search for relavent research papers and use their algorithm, but you must <b>give proper credits</b> by citing them in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "background = cv2.imread('./image.jpg')\n",
    "\n",
    "videoFrames = []\n",
    "while cap.isOpened():\n",
    "    #caturing the live frame\n",
    "    ret, current_frame = cap.read()\n",
    "    if ret:\n",
    "        #converting from rgb to hsv color space\n",
    "        hsv_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #range for lower red\n",
    "        l_red = np.array([5,76,100])\n",
    "        u_red = np.array([44,255,255])\n",
    "        mask1 = cv2.inRange(hsv_frame, l_red, u_red)\n",
    "\n",
    "        #generating the final red mask\n",
    "        red_mask = mask1\n",
    "\n",
    "        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8), iterations = 10) \n",
    "        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_DILATE, np.ones((3,3), np.uint8), iterations = 1)  \n",
    "\n",
    "        #subsituting the red portion with backgrpound image\n",
    "        part1 = cv2.bitwise_and(background, background, mask= red_mask)\n",
    "        \n",
    "        # detecting things which are not red\n",
    "        red_free = cv2.bitwise_not(red_mask)\n",
    "\n",
    "        # if cloak is not present show the current image\n",
    "        part2 = cv2.bitwise_and(current_frame, current_frame, mask= red_free)\n",
    "\n",
    "        videoFrames.append(part1+part2)\n",
    "\n",
    "        #final output\n",
    "        cv2.imshow(\"cloak\", part1 + part2)\n",
    "        if cv2.waitKey(5) == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "\n",
    "inputWidth, inputHeight = videoFrames[0].shape[1], videoFrames[0].shape[0]\n",
    "outputStream = cv2.VideoWriter('cloak.mp4',\n",
    "                               cv2.VideoWriter_fourcc('x', '2', '6', '4'),\n",
    "                               25, (inputWidth, inputHeight))\n",
    "\n",
    "for frame in videoFrames:\n",
    "        # Write frame to outputStream\n",
    "        outputStream.write(frame)\n",
    "        # Encode image as jpg numpy array\n",
    "        clear_output(wait=True)\n",
    "outputStream.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
